---
id: dl-09
topic: Deep Learning
difficulty: Medium
title: ReLU Logic
type: MCQ
companyTags: [Standard]
acceptanceRate: 85
---

# Scenario

What does the ReLU (Rectified Linear Unit) function do?

## Options

- `It makes negative numbers zero, and leaves positive numbers unchanged.`
- `It converts everything to probabilities (0-1).`
- `It squares the input.`
- `It randomly drops neurons.`

## Correct Answer

```
It makes negative numbers zero, and leaves positive numbers unchanged.
```

## Explanation

`f(x) = max(0, x)`. It is computationally fast and solves many gradient problems compared to Sigmoid.
